{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_words(text):\n",
    "    # split into words by white space\n",
    "    words = text.split()\n",
    "    for word_index in range(len(words)):\n",
    "        if '’' in words[word_index]:\n",
    "            words[word_index] = words[word_index].replace('’','')\n",
    "    return words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    stripped = [re_punc.sub('', w) for w in words]\n",
    "    return stripped\n",
    "\n",
    "def keep_alphabetic(words):\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    return words\n",
    "\n",
    "def to_sentence(words):\n",
    "    # join words to a sentence\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tweet(words):\n",
    "    tweet_tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True,reduce_len=True)\n",
    "    tweet = tweet_tokenizer.tokenize(words)\n",
    "    return tweet\n",
    "\n",
    "def denoise_text(text):\n",
    "        words = split_into_words(text)\n",
    "        words = remove_punctuation(words)\n",
    "        words = keep_alphabetic(words)\n",
    "        return to_sentence(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10928\\1934443529.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tweets'][i] = denoise_text(text)\n"
     ]
    }
   ],
   "source": [
    "def preparation_text():\n",
    "    # #Removing the noisy word from text\n",
    "    df = pd.read_csv('../data/tweet.csv')\n",
    "    i=0\n",
    "    hashtag_arr=[]\n",
    "    for text in df['Tweets']:\n",
    "        splitted_text=text.split()\n",
    "        hashtag = [word for word in splitted_text if word.startswith(\"#\")]\n",
    "        \n",
    "        \n",
    "        hashtag = '  '.join(hashtag)\n",
    "        hashtag_arr.append(hashtag)\n",
    "        df['Tweets'][i] = denoise_text(text)\n",
    "        i+=1\n",
    "        \n",
    "    df['hashtag'] = hashtag_arr\n",
    "    df.set_index('Tweets', inplace=True)\n",
    "    df.to_csv('../data/tweet.csv')\n",
    "    \n",
    "preparation_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Name</th>\n",
       "      <th>data</th>\n",
       "      <th>location</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From decreasing maternal mortality to lowering...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-18 18:51:11+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>183</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa has always been at the heart of our fou...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-17 20:27:13+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1587</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AmrefWorldwide is doing great things for healt...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-17 20:05:19+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>72</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Many people dont know that Mike was a champion...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-17 19:07:18+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>805</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im very sad to hear about Michael Gersons pass...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-17 19:07:17+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1293</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>174</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KenyaGovernors gatesfoundation It was a pleasu...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-17 18:36:48+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT Africacom is excited to host Bill Gates in ...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-16 20:55:31+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Im in Kenya this week to learn from researcher...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-16 20:18:54+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>17451</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>1795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thank you for having me Im excited to be here ...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-16 19:36:58+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>4809</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>480</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OfficialMakueni Thank you I enjoyed learning f...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-16 19:34:34+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I believe everyone has a role to play in addre...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-15 18:55:28+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2657</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>404</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>When it comes to alleviating the impacts of cl...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-14 02:24:53+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1961</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>288</td>\n",
       "      <td>#climatechange  #COP27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Those of us who have done the most to cause th...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-12 23:37:19+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2454</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>More than children die from pneumonia every ye...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-12 16:38:54+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1890</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>268</td>\n",
       "      <td>#WorldPneumoniaDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>While the last steps are proving to be the tou...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-10 22:00:16+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2440</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The climate crisis is already devastating comm...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-07 23:38:09+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1989</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>319</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I have never met anyone who was more passionat...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-04 16:49:54+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1962</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>More than people globally still suffer from NT...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-03 21:47:29+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1773</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>236</td>\n",
       "      <td>#NTDs,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This years brought together investors policyma...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-11-03 17:09:17+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1565</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>222</td>\n",
       "      <td>#BESummit2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Math shouldnt be a gatekeeper limiting a stude...</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>2022-10-28 16:42:42+00:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>3559</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-06-24 18:44:10+00:00</td>\n",
       "      <td>518</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweets        Name  \\\n",
       "0   From decreasing maternal mortality to lowering...  Bill Gates   \n",
       "1   Africa has always been at the heart of our fou...  Bill Gates   \n",
       "2   AmrefWorldwide is doing great things for healt...  Bill Gates   \n",
       "3   Many people dont know that Mike was a champion...  Bill Gates   \n",
       "4   Im very sad to hear about Michael Gersons pass...  Bill Gates   \n",
       "5   KenyaGovernors gatesfoundation It was a pleasu...  Bill Gates   \n",
       "6   RT Africacom is excited to host Bill Gates in ...  Bill Gates   \n",
       "7   Im in Kenya this week to learn from researcher...  Bill Gates   \n",
       "8   Thank you for having me Im excited to be here ...  Bill Gates   \n",
       "9   OfficialMakueni Thank you I enjoyed learning f...  Bill Gates   \n",
       "10  I believe everyone has a role to play in addre...  Bill Gates   \n",
       "11  When it comes to alleviating the impacts of cl...  Bill Gates   \n",
       "12  Those of us who have done the most to cause th...  Bill Gates   \n",
       "13  More than children die from pneumonia every ye...  Bill Gates   \n",
       "14  While the last steps are proving to be the tou...  Bill Gates   \n",
       "15  The climate crisis is already devastating comm...  Bill Gates   \n",
       "16  I have never met anyone who was more passionat...  Bill Gates   \n",
       "17  More than people globally still suffer from NT...  Bill Gates   \n",
       "18  This years brought together investors policyma...  Bill Gates   \n",
       "19  Math shouldnt be a gatekeeper limiting a stude...  Bill Gates   \n",
       "\n",
       "                         data     location  favorite_count  verified  \\\n",
       "0   2022-11-18 18:51:11+00:00  Seattle, WA             183      True   \n",
       "1   2022-11-17 20:27:13+00:00  Seattle, WA            1587      True   \n",
       "2   2022-11-17 20:05:19+00:00  Seattle, WA              72      True   \n",
       "3   2022-11-17 19:07:18+00:00  Seattle, WA             805      True   \n",
       "4   2022-11-17 19:07:17+00:00  Seattle, WA            1293      True   \n",
       "5   2022-11-17 18:36:48+00:00  Seattle, WA              23      True   \n",
       "6   2022-11-16 20:55:31+00:00  Seattle, WA               0      True   \n",
       "7   2022-11-16 20:18:54+00:00  Seattle, WA           17451      True   \n",
       "8   2022-11-16 19:36:58+00:00  Seattle, WA            4809      True   \n",
       "9   2022-11-16 19:34:34+00:00  Seattle, WA              28      True   \n",
       "10  2022-11-15 18:55:28+00:00  Seattle, WA            2657      True   \n",
       "11  2022-11-14 02:24:53+00:00  Seattle, WA            1961      True   \n",
       "12  2022-11-12 23:37:19+00:00  Seattle, WA            2454      True   \n",
       "13  2022-11-12 16:38:54+00:00  Seattle, WA            1890      True   \n",
       "14  2022-11-10 22:00:16+00:00  Seattle, WA            2440      True   \n",
       "15  2022-11-07 23:38:09+00:00  Seattle, WA            1989      True   \n",
       "16  2022-11-04 16:49:54+00:00  Seattle, WA            1962      True   \n",
       "17  2022-11-03 21:47:29+00:00  Seattle, WA            1773      True   \n",
       "18  2022-11-03 17:09:17+00:00  Seattle, WA            1565      True   \n",
       "19  2022-10-28 16:42:42+00:00  Seattle, WA            3559      True   \n",
       "\n",
       "                   created_at  retweet_count                 hashtag  \n",
       "0   2009-06-24 18:44:10+00:00             27                     NaN  \n",
       "1   2009-06-24 18:44:10+00:00            253                     NaN  \n",
       "2   2009-06-24 18:44:10+00:00             21                     NaN  \n",
       "3   2009-06-24 18:44:10+00:00             74                     NaN  \n",
       "4   2009-06-24 18:44:10+00:00            174                     NaN  \n",
       "5   2009-06-24 18:44:10+00:00              2                     NaN  \n",
       "6   2009-06-24 18:44:10+00:00            126                     NaN  \n",
       "7   2009-06-24 18:44:10+00:00           1795                     NaN  \n",
       "8   2009-06-24 18:44:10+00:00            480                     NaN  \n",
       "9   2009-06-24 18:44:10+00:00              9                     NaN  \n",
       "10  2009-06-24 18:44:10+00:00            404                     NaN  \n",
       "11  2009-06-24 18:44:10+00:00            288  #climatechange  #COP27  \n",
       "12  2009-06-24 18:44:10+00:00            431                     NaN  \n",
       "13  2009-06-24 18:44:10+00:00            268      #WorldPneumoniaDay  \n",
       "14  2009-06-24 18:44:10+00:00            231                     NaN  \n",
       "15  2009-06-24 18:44:10+00:00            319                     NaN  \n",
       "16  2009-06-24 18:44:10+00:00            270                     NaN  \n",
       "17  2009-06-24 18:44:10+00:00            236                  #NTDs,  \n",
       "18  2009-06-24 18:44:10+00:00            222           #BESummit2022  \n",
       "19  2009-06-24 18:44:10+00:00            518                     NaN  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/tweet.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [286], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mTweets\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m----> 2\u001b[0m     common_words \u001b[39m=\u001b[39m get_top_n_words(text, \u001b[39m10\u001b[39m)\n\u001b[0;32m      3\u001b[0m     df2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(common_words,columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m     df2\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msum()[\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [285], line 6\u001b[0m, in \u001b[0;36mget_top_n_words\u001b[1;34m(corpus, n)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_top_n_words\u001b[39m(corpus, n\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m----> 6\u001b[0m     vec \u001b[39m=\u001b[39m CountVectorizer()\u001b[39m.\u001b[39;49mfit(corpus)\n\u001b[0;32m      7\u001b[0m     bag_of_words \u001b[39m=\u001b[39m vec\u001b[39m.\u001b[39mtransform(corpus)\n\u001b[0;32m      8\u001b[0m     sum_words \u001b[39m=\u001b[39m bag_of_words\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1291\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \n\u001b[0;32m   1277\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[39m    Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m-> 1291\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   1292\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1317\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[39m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[39m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[39m# TfidfVectorizer.\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 1317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1318\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1319\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_vocabulary()\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "for text in df['Tweets']:\n",
    "    common_words = get_top_n_words(text, 10)\n",
    "    df2 = pd.DataFrame(common_words,columns=['word','count'])\n",
    "    df2.groupby('word').sum()['count'].sort_values(ascending=False)\n",
    "    fig=px.bar(df2,x='word',y='count',color='count',title='Top 10 unigrams')\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2195721ce15285787f06a34273eb17bd63f63c624f05556bec3de0873b93cb23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
